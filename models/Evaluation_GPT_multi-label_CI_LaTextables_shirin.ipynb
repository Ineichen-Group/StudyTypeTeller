{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e29cb7f-b800-4a1e-ac28-6dc9a5cb3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1334eb11-1c4a-4532-91e4-808555999fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confidenceinterval import classification_report_with_ci\n",
    "from confidenceinterval.bootstrap import bootstrap_ci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cf73e-7eae-4806-a825-323ff85fcf39",
   "metadata": {},
   "source": [
    "# Evaluate Multi-Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08929a5-9749-4c3a-a432-88c3f25258d5",
   "metadata": {},
   "source": [
    "## Load and combine multiple files with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b064e86b-d3b0-4332-b40d-b7248a095a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of the sub-lists will determine the reading of the relevant files\n",
    "prompt_ids_to_eval = [[\"P1\"], [\"P2\"], [\"P3_1\"], [\"P3_2\", \"P3_3\", \"P3_4\"], [\"P4_1\", \"P4_2\"], [\"P5\", \"P6\", \"P7\"], [\"P9\", \"P9_1\"], [\"P10\", \"P11\"], [\"P11_1\", \"P11_2\", \"P11_3\"], [\"P11_4\", \"P11_5\"], [\"P12\"], [\"P12_1\", \"P12_2\"]]\n",
    "\n",
    "model = \"gpt-3.5-turbo\" #\"gpt-3.5-turbo\" \"gpt-4-turbo-preview\"\n",
    "data_type = \"enriched_kw\"\n",
    "#data_type = \"enriched\"\n",
    "#data_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8c3c77-e9ee-441f-989b-8729208a21ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m all_prediction_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Process the first sublist to initialize the big DataFrame\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df, prediction_columns \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_prompt_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids_to_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Add the prediction columns to the set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m all_prediction_columns\u001b[38;5;241m.\u001b[39mupdate(prediction_columns)\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mprocess_prompt_ids\u001b[0;34m(prompt_ids)\u001b[0m\n\u001b[1;32m      8\u001b[0m columns_to_read \u001b[38;5;241m=\u001b[39m basic_columns \u001b[38;5;241m+\u001b[39m prediction_columns\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#df = pd.read_csv(f\"predictions/{model}_{data_type}_test_outputs_{'_'.join(prompt_ids)}.csv\")[columns_to_read] #for multi-label\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_outputs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids_to_eval\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hierarchical.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)[columns_to_read] \u001b[38;5;66;03m#for hierarchical\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df, prediction_columns\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "# Define a function to process each sublist\n",
    "def process_prompt_ids(prompt_ids):\n",
    "    # Create the list of prediction columns based on the prompt IDs\n",
    "    prediction_columns = [f'gpt_predictions_{prompt_id}' for prompt_id in prompt_ids]\n",
    "    # Specify the basic columns to include in the DataFrame\n",
    "    basic_columns = ['pmid', 'accepted_label', 'multi_label', 'binary_label']\n",
    "    # Combine basic columns with the dynamically generated prediction columns\n",
    "    columns_to_read = basic_columns + prediction_columns\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(f\"predictions/{model}_{data_type}_test_outputs_{'_'.join(prompt_ids)}.csv\")[columns_to_read] #for multi-label\n",
    "    return df, prediction_columns\n",
    "    \n",
    "# Initialize an empty set to store all prediction columns\n",
    "all_prediction_columns = set()\n",
    "\n",
    "# Process the first sublist to initialize the big DataFrame\n",
    "df, prediction_columns = process_prompt_ids(prompt_ids_to_eval[0])\n",
    "# Add the prediction columns to the set\n",
    "all_prediction_columns.update(prediction_columns)\n",
    "\n",
    "# Iterate over the remaining sublists\n",
    "for prompt_ids in prompt_ids_to_eval[1:]:\n",
    "    # Process the current sublist\n",
    "    df_single_file, prediction_columns = process_prompt_ids(prompt_ids)\n",
    "    # Merge the big DataFrame with the current DataFrame on 'pmid' using a left join\n",
    "    df = pd.merge(df, df_single_file[['pmid'] + prediction_columns], on='pmid', how='left')\n",
    "    # Keep only the prediction columns from the current DataFrame\n",
    "    #big_df = big_df[['pmid', 'accepted_label', 'multi_label', 'binary_label'] + prediction_columns]\n",
    "    # Add the prediction columns to the set\n",
    "    all_prediction_columns.update(prediction_columns)\n",
    "\n",
    "# Convert the set of prediction columns to a list\n",
    "all_prediction_columns = list(all_prediction_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dd7e9-bb4e-47d5-9862-5d547e0cabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_columns = all_prediction_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efda7bc-e2e8-4cc7-bc4d-741d553402b6",
   "metadata": {},
   "source": [
    "## Map predictions to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860b2b-86a3-4fc3-bd0e-1556a2ac89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_numerical = {\n",
    "    'Remaining': 0,\n",
    "    'Non-systematic-review': 1,\n",
    "    'Human-non-RCT-non-drug-intervention': 2,\n",
    "    'Human-non-RCT-drug-intervention': 3,\n",
    "    'Human-case-report': 4,\n",
    "    'Animal-other': 5,\n",
    "    'Animal-drug-intervention': 6,\n",
    "    'Human-systematic-review': 7,\n",
    "    'In-vitro-study': 8,\n",
    "    'Human-RCT-non-drug-intervention': 9,\n",
    "    'Animal-non-drug-intervention': 10,\n",
    "    'Human-RCT-drug-intervention': 11,\n",
    "    'Clinical-study-protocol': 12,\n",
    "    'Human-RCT-non-intervention': 13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf7b15-4a76-46f6-a687-19952dd8c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_to_label = {v: f\"{k}\" for k, v in label_to_numerical.items()}\n",
    "numerical_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657b963-b35c-4d0a-9b5d-aa8ddeb267d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def map_label_fuzzy(label, label_dict):\n",
    "    #label = label.lower().replace('-', '').replace('_', ' ').strip()\n",
    "    best_match = difflib.get_close_matches(label, label_dict.keys(), n=1, cutoff=0.6)\n",
    "    if best_match:\n",
    "        #print(f'{label} matched to {best_match[0]}')\n",
    "        return label_dict[best_match[0]]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5de33-4fc2-40b9-adfe-b972b17f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label_fuzzy(\"RCT-drug-intervention\", label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ae98-24fd-427f-b649-edbb5718fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numerical(label, label_dict):\n",
    "    # Check if label is a dictionary\n",
    "    if isinstance(label, dict):\n",
    "        # Extract the label with the highest score/probability\n",
    "        highest_label = max(normalized_label, key=label.get)\n",
    "        return label_dict.get(highest_label, -1)\n",
    "    else:\n",
    "         # Normalize label\n",
    "        normalized_label = label.replace(',', '').strip().replace(' ', '-')#.strip()\n",
    "        # Directly map string labels to numerical IDs\n",
    "        numerical_label = label_dict.get(label, -1)\n",
    "        # Fuzzy match if no direct mapping possible\n",
    "        if numerical_label == -1:\n",
    "            numerical_label = map_label_fuzzy(normalized_label, label_dict)\n",
    "        # If fuzzy match did not work, check if the label string contains the key 'label' and use it to split the string; keep everything to the right as the potential label\n",
    "        if numerical_label == -1:\n",
    "            if 'label' in label:\n",
    "                label_part = label.split('label')[1]\n",
    "                numerical_label = map_label_fuzzy(label_part, label_dict)\n",
    "        return numerical_label\n",
    "        \n",
    "# Convert accepted labels to numerical\n",
    "df['accepted_label_numerical'] = df['accepted_label'].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "\n",
    "for col in prediction_columns:\n",
    "    df[f'{col}_numerical'] = df[col].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b5567-d404-49d6-85a5-bf717a3910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_list_of_lists(lst):\n",
    "    return all(isinstance(sublist, list) for sublist in lst)\n",
    "\n",
    "if is_list_of_lists(prompt_ids_to_eval):\n",
    "    # Flatten the list of lists\n",
    "    prompt_ids_to_eval_flat = [item for sublist in prompt_ids_to_eval for item in sublist]\n",
    "    # Create a string suffix for the CSV file name\n",
    "    csv_file_suffix = '_'.join(prompt_ids_to_eval_flat)\n",
    "else:\n",
    "    csv_file_suffix = '_'.join(prompt_ids_to_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df01853-49d5-4b30-80a5-73e680e60c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_suffix = 'all_prompts' # when many prompts were loaded, the file name becomes too long and cannot be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224cc72-c698-429e-8efa-53bbb94fe96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(f\"predictions/{model}_{data_type}test_outputs_{'_'.join(csv_file_suffix)}_structured.csv\")\n",
    "df.to_csv(f\"predictions/{model}_{data_type}_test_outputs_{'_'.join(csv_file_suffix)}_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5fbb-23c4-48b8-be3b-a700d0667854",
   "metadata": {},
   "source": [
    "#### Important: some labels from GPT could not be mapped to a target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a4b8-fe40-4b75-aa4b-47874029ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_minus_one = df[(df == -1).any(axis=1)]\n",
    "rows_with_minus_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c211f76-a138-41a2-a16a-97cdaf21a5bd",
   "metadata": {},
   "source": [
    "## Evaluate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f897e-17c7-4005-a2bd-891f13baa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, csv_file_suffix=None, digits=3):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report_with_ci(y_true, y_pred, numerical_to_label_map=numerical_to_label, round_ndigits = digits)\n",
    "\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report)\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        report_df.set_index('class', inplace=True)\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_{data_type}_{prompt_id}_{eval_type}_ci.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "    if not csv_file_suffix:\n",
    "        csv_file_suffix = '_'.join(prompt_ids_to_eval) + \"_\" + eval_type\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_{data_type}_test_per_class_{csv_file_suffix}_with_ci.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_{data_type}_test_summary_{csv_file_suffix}_with_ci.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2c904-2156-4069-9141-398732642c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_list_of_lists(prompt_ids_to_eval):\n",
    "    # Flatten the list of lists\n",
    "    prompt_ids_to_eval = [item for sublist in prompt_ids_to_eval for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffb7b9-01c5-4398-affd-e35528b56534",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_suffix # control that is correct one= \"all prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875e085-c945-4e93-a6af-3b7cbc6d9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_col = 'accepted_label_numerical'\n",
    "#eval_type = 'multi_label'\n",
    "eval_type = 'hierarchical'\n",
    "all_reports_df, summary_df = evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, csv_file_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d603cc-2300-4562-8e8d-cde88c2baabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6172fa-7032-47ec-b183-4a40a23f5649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_reports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444288b6-8dac-41a3-83aa-a392cfc546ad",
   "metadata": {},
   "source": [
    "### Format Results and Generate LateX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b455b5d-a4d5-4ebe-ba85-3d666fc5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"gpt-4-turbo-preview\"\n",
    "\n",
    "#prompt_ids_to_eval = [\"P6\", \"P7\", \"P11_3\", \"P11_4\"]\n",
    "#summary_gpt4_raw_1 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P1\", \"P4_1\", \"P5\"]\n",
    "#summary_gpt4_raw_2 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P12_2\"]\n",
    "#summary_gpt4_raw_3 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#summary_gpt4_raw = pd.concat([summary_gpt4_raw_1,summary_gpt4_raw_2, summary_gpt4_raw_3]) \n",
    "#summary_gpt4_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8926a8-1b9d-4ebc-bfe9-8640a6cdc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case of combining separately evaluated prompt files\n",
    "\n",
    "#model = \"gpt-3.5-turbo\"\n",
    "\n",
    "#prompt_ids_to_eval = [\"P5\"]\n",
    "#summary_gpt3_raw_1 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P6\"]\n",
    "#summary_gpt3_raw_2 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P1\"]\n",
    "#summary_gpt3_raw_3 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P7\"]\n",
    "#summary_gpt3_raw_4 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P3_3\",\"P3_4\",\"P4_1\",\"P4_2\"]\n",
    "#summary_gpt3_raw_5 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P11\",\"P11_1\",\"P11_2\",\"P11_3\"]\n",
    "#summary_gpt3_raw_6 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P12_2\"]\n",
    "#summary_gpt3_raw_7 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#prompt_ids_to_eval = [\"P11_4\",\"P11_5\",\"P12\"]\n",
    "#summary_gpt3_raw_8 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "#summary_gpt3_raw = pd.concat([summary_gpt3_raw_1,summary_gpt3_raw_2, summary_gpt3_raw_3, summary_gpt3_raw_4, summary_gpt3_raw_5, summary_gpt3_raw_6, summary_gpt3_raw_7, summary_gpt3_raw_8]) \n",
    "#summary_gpt3_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6da0ca-e5f3-4943-955b-1e302a032e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case when the prompt files were combined before the evaluation and the evaluation is already in a single file\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "csv_file_suffix=\"all_prompts\"\n",
    "summary_gpt3_raw = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_{csv_file_suffix}_with_ci.csv\", index_col=0)\n",
    "summary_gpt3_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20048cfe-b061-4726-9a0a-f7afb6a94e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_to_concept = {\n",
    "    'P1': 'zero-shot',\n",
    "    'P2': 'zero-shot',\n",
    "    'P3_1': 'CC',\n",
    "    'P3_2': 'CC',\n",
    "    'P4_1': 'CC',\n",
    "    'P4_2': 'CC',\n",
    "    'P5': 'CC',\n",
    "    'P6': 'CC',\n",
    "    'P7': 'CoT',\n",
    "    'P9': 'CoT + CC',\n",
    "    'P9_1': 'CoT + CC',\n",
    "    'P10': 'CoT',\n",
    "    'P11': 'CoT + CC',\n",
    "    'P11_1': 'CoT + CC',\n",
    "    'P11_2': 'CoT + CC',\n",
    "    'P11_3': 'CoT + CC',\n",
    "    'P11_4': 'CoT + CC',\n",
    "    'P11_5': 'CoT + CC',\n",
    "    'P12': '2 CoT + CC',\n",
    "    'P12_1': '2 CoT + CC',\n",
    "    'P12_2': '2 CoT + CC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fe17b-3ccb-4d31-8526-104d5bb29cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sorting function\n",
    "def custom_sort(prompt):\n",
    "    import re\n",
    "    # Extract numbers from the prompt\n",
    "    numbers = re.findall(r'\\d+', prompt)\n",
    "    if numbers:\n",
    "        # Primary sort by the first number, secondary sort by the full string\n",
    "        return (int(numbers[0]), prompt)\n",
    "    return (float('inf'), prompt)  # Handles cases without numbers\n",
    "    \n",
    "def format_summary_for_latex_report(summary_df, map_prompt_to_concept):\n",
    "    summary_df = summary_df.copy()\n",
    "    \n",
    "    # Concatenating each metric with its CI\n",
    "    summary_df['Precision (CI)'] = summary_df['precision'].astype(str) + ' ' + summary_df['precision CI'].astype(str)\n",
    "    summary_df['Recall (CI)'] = summary_df['recall'].astype(str) + ' ' + summary_df['recall CI'].astype(str)\n",
    "    summary_df['F1-Score (CI)'] = summary_df['f1-score'].astype(str) + ' ' + summary_df['f1-score CI'].astype(str)\n",
    "    summary_df['Accuracy (CI)'] = summary_df['accuracy'].astype(str) + ' ' + summary_df['accuracy CI'].astype(str)\n",
    "    \n",
    "    # Dropping old columns\n",
    "    summary_df.drop(columns=['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI'], inplace=True)\n",
    "    \n",
    "    # Rename 'Prompt ID' to 'Prompt'\n",
    "    summary_df.rename(columns={'Prompt ID': 'Prompt'}, inplace=True)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    summary_df['Concept'] = summary_df['Prompt'].map(map_prompt_to_concept)\n",
    "    \n",
    "    # Rearrange the columns to put 'Concept' after 'Prompt'\n",
    "    summary_df = summary_df[['Prompt', 'Concept', 'Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)']]\n",
    "\n",
    "    summary_df['sort_key'] = summary_df['Prompt'].apply(custom_sort)\n",
    "    summary_df.sort_values('sort_key', inplace=True)\n",
    "    summary_df.drop('sort_key', inplace=True, axis=1)\n",
    "    \n",
    "    return summary_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bf3c8-6e45-4e4e-bb20-a6808ab4868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_gpt4 = format_summary_for_latex_report(summary_gpt4_raw, map_prompt_to_concept)\n",
    "#summary_gpt4['Model'] = 'gpt-4'\n",
    "#summary_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739ef44-99ea-42c0-bb52-35310f45669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summary_gpt4.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ada948-fafd-4659-b5aa-8837bf6d6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt3 = format_summary_for_latex_report(summary_gpt3_raw, map_prompt_to_concept)\n",
    "summary_gpt3['Model'] = 'gpt-3.5'\n",
    "summary_gpt3 = summary_gpt3.dropna(subset=['Concept'])\n",
    "summary_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef8548-4ca7-4f6c-91b9-29243ab01f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt3.drop(columns=['Model', 'Accuracy (CI)'], inplace=True)\n",
    "\n",
    "print(summary_gpt3.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be20eb-da26-491c-988c-5b21752fe3de",
   "metadata": {},
   "source": [
    "### combine gpt-3.5 and gpt-4 results in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10feeb6-3188-4b5e-9d5c-4922d8a77108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames\n",
    "#combined_df = pd.concat([summary_gpt4, summary_gpt3])\n",
    "\n",
    "# Pivot table with multi-level columns for metrics and models as subcolumns\n",
    "#pivot_df = combined_df.pivot_table(\n",
    "    #index=['Prompt', 'Concept'],\n",
    "    #columns='Model',\n",
    "    #values=['Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)'],\n",
    "    #aggfunc='first'\n",
    ")\n",
    "\n",
    "# Simplify the MultiIndex in columns\n",
    "#pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
    "#pivot_df = pivot_df.reset_index()\n",
    "\n",
    "#pivot_df['sort_key'] = pivot_df['Prompt'].apply(custom_sort)\n",
    "#pivot_df.sort_values('sort_key', inplace=True)\n",
    "#pivot_df.drop('sort_key', inplace=True, axis=1)\n",
    "\n",
    "#pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660ee6c-05aa-4fd7-b42c-3de4d6580dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot_df = pd.DataFrame(pivot_df).drop(columns=['Accuracy (CI) gpt-3.5', 'Accuracy (CI) gpt-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd9acf-1252-4da3-8229-c035164beca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MultiIndex for columns based on model and metric\n",
    "#new_columns = [\n",
    "    ##('Accuracy', 'gpt-3.5'), ('Accuracy', 'gpt-4'),\n",
    "    #('Precision', 'gpt-3.5'), ('Precision', 'gpt-4'),\n",
    "    #('Recall', 'gpt-3.5'), ('Recall', 'gpt-4'),\n",
    "    #('F1-Score', 'gpt-3.5'), ('F1-Score', 'gpt-4')\n",
    "]\n",
    "\n",
    "# Define the new column index as a MultiIndex\n",
    "#multi_index = pd.MultiIndex.from_tuples(new_columns, names=['Metric', 'Model'])\n",
    "\n",
    "# Create a new DataFrame using only the relevant columns and assign the MultiIndex\n",
    "#new_df = pd.DataFrame(pivot_df, columns=['Prompt', 'Concept', \n",
    "                                   ##'Accuracy (CI) gpt-3.5', 'Accuracy (CI) gpt-4',\n",
    "                                   #'Precision (CI) gpt-3.5', 'Precision (CI) gpt-4',\n",
    "                                   #'Recall (CI) gpt-3.5', 'Recall (CI) gpt-4',\n",
    "                                   #'F1-Score (CI) gpt-3.5', 'F1-Score (CI) gpt-4',])\n",
    "\n",
    "# Rename columns to match the MultiIndex\n",
    "#new_df.columns = ['Prompt', 'Concept'] + multi_index.to_flat_index().tolist()\n",
    "\n",
    "# Set the new column index\n",
    "#new_df.columns = pd.MultiIndex.from_tuples([('', 'Prompt'), ('', 'Concept')] + new_columns)\n",
    "\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf6133-90ea-4dba-b0d5-5e4d5959d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatter function to convert float to percentage\n",
    "#def to_percentage(x):\n",
    "    #return \"{:.1f}%\".format(x * 100)\n",
    "\n",
    "# Define column formatters\n",
    "#formatters = {\n",
    "    #'Prompt': lambda x: x.replace('_', r'\\_'),\n",
    "    #'Accuracy (gpt-3.5)': to_percentage,\n",
    "    #'Accuracy (gpt-4)': to_percentage,\n",
    "    #'F1-Score (gpt-3.5)': to_percentage,\n",
    "    #'F1-Score (gpt-4)': to_percentage,\n",
    "    #'Precision (gpt-3.5)': to_percentage,\n",
    "    #'Precision (gpt-4)': to_percentage,\n",
    "    #'Recall (gpt-3.5)': to_percentage,\n",
    "    #'Recall (gpt-4)': to_percentage,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3447f8b-fe19-4b46-80b4-d7a9b49b8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_df.to_latex(float_format=\"%.3f\", index=False, formatters={'': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a47553-3260-4e75-8980-3188c2a2eb09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d528d-c7ea-4d90-be74-49d44e3519e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodigy",
   "language": "python",
   "name": "prodigy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
