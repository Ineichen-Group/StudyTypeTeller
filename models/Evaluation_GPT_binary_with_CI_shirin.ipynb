{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e861e18f-284f-4bd3-a15d-c2d83f82f090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62cf70b9-0b24-44b1-9f09-b4537aa968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confidenceinterval import classification_report_with_ci\n",
    "from confidenceinterval.bootstrap import bootstrap_ci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23737756-37bc-4ece-8f25-d613023365a1",
   "metadata": {},
   "source": [
    "# Evaluate Multi-Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1f875-9e49-4012-ad47-80c82660e8c5",
   "metadata": {},
   "source": [
    "## Load individual file with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb26c4e-9f42-4762-82b9-dbfbb73915d3",
   "metadata": {},
   "source": [
    "Skip the below if you have already loaded multiple files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fac99f9a-f33f-444c-b35f-75e9de6f4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids_to_eval = [\"P1_HIERARCHY\",\"P2_HIERARCHY\"] #[\"P1\", \"P4_1\", \"P5\"] # [\"P6\", \"P7\", \"P11_3\", \"P11_4\"]\n",
    "model = \"gpt-3.5-turbo\" #\"gpt-3.5-turbo\" \"gpt-4-turbo-preview\"\n",
    "data_type = \"enriched_kw\" # change when evaluating enriched without kw/ not enriched to be able to load and save the correct names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3451a2a7-aa01-4423-b816-d628e324ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of prediction columns based on the prompt IDs\n",
    "prediction_columns = [f'gpt_predictions_{prompt_id}' for prompt_id in prompt_ids_to_eval]\n",
    "# Specify the basic columns to include in the DataFrame\n",
    "basic_columns = ['pmid', 'accepted_label', 'multi_label', 'binary_label']\n",
    "# Combine basic columns with the dynamically generated prediction columns\n",
    "columns_to_read = basic_columns + prediction_columns\n",
    "\n",
    "df = pd.read_csv(f\"predictions/{model}_{data_type}_test_outputs_P2_{'_'.join(prompt_ids_to_eval)}_hierarchical.csv\")[columns_to_read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e4415f3-b7d4-4b30-88db-2381d25714f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pmid',\n",
       " 'accepted_label',\n",
       " 'multi_label',\n",
       " 'binary_label',\n",
       " 'gpt_predictions_P1_HIERARCHY',\n",
       " 'gpt_predictions_P2_HIERARCHY']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3f4d208e-7607-4467-8bee-642018fea708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 6)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c421e880-ddc3-4dc5-be01-72f97a9ee980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>accepted_label</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>gpt_predictions_P1_HIERARCHY</th>\n",
       "      <th>gpt_predictions_P2_HIERARCHY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32147509</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-systematic-review</td>\n",
       "      <td>Animal-systematic-review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8800803</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23811310</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36314672</td>\n",
       "      <td>Remaining</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11909745</td>\n",
       "      <td>Remaining</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid         accepted_label  multi_label  binary_label  \\\n",
       "0  32147509  Non-systematic-review            1             0   \n",
       "1   8800803  Non-systematic-review            1             0   \n",
       "2  23811310  Non-systematic-review            1             0   \n",
       "3  36314672              Remaining            0             0   \n",
       "4  11909745              Remaining            0             0   \n",
       "\n",
       "  gpt_predictions_P1_HIERARCHY gpt_predictions_P2_HIERARCHY  \n",
       "0     Animal-systematic-review     Animal-systematic-review  \n",
       "1                 Animal-other                 Animal-other  \n",
       "2                 Animal-other                 Animal-other  \n",
       "3                 Animal-other                 Animal-other  \n",
       "4                 Animal-other                 Animal-other  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae89110-d694-4650-8b3c-81e46a8b6471",
   "metadata": {},
   "source": [
    "## Map predictions to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de860b2b-86a3-4fc3-bd0e-1556a2ac89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_numerical = {\n",
    "    'Remaining': 0,\n",
    "    'Non-systematic-review': 1,\n",
    "    'Human-non-RCT-non-drug-intervention': 2,\n",
    "    'Human-non-RCT-drug-intervention': 3,\n",
    "    'Human-case-report': 4,\n",
    "    'Animal-other': 5,\n",
    "    'Animal-drug-intervention': 6,\n",
    "    'Human-systematic-review': 7,\n",
    "    'In-vitro-study': 8,\n",
    "    'Human-RCT-non-drug-intervention': 9,\n",
    "    'Animal-non-drug-intervention': 10,\n",
    "    'Human-RCT-drug-intervention': 11,\n",
    "    'Clinical-study-protocol': 12,\n",
    "    'Human-RCT-non-intervention': 13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33bf7b15-4a76-46f6-a687-19952dd8c57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Remaining',\n",
       " 1: 'Non-systematic-review',\n",
       " 2: 'Human-non-RCT-non-drug-intervention',\n",
       " 3: 'Human-non-RCT-drug-intervention',\n",
       " 4: 'Human-case-report',\n",
       " 5: 'Animal-other',\n",
       " 6: 'Animal-drug-intervention',\n",
       " 7: 'Human-systematic-review',\n",
       " 8: 'In-vitro-study',\n",
       " 9: 'Human-RCT-non-drug-intervention',\n",
       " 10: 'Animal-non-drug-intervention',\n",
       " 11: 'Human-RCT-drug-intervention',\n",
       " 12: 'Clinical-study-protocol',\n",
       " 13: 'Human-RCT-non-intervention'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_to_label = {v: f\"{k}\" for k, v in label_to_numerical.items()}\n",
    "numerical_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8657b963-b35c-4d0a-9b5d-aa8ddeb267d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def map_label_fuzzy(label, label_dict):\n",
    "    #label = label.lower().replace('-', '').replace('_', ' ').strip()\n",
    "    best_match = difflib.get_close_matches(label, label_dict.keys(), n=1, cutoff=0.6)\n",
    "    if best_match:\n",
    "        #print(f'{label} matched to {best_match[0]}')\n",
    "        return label_dict[best_match[0]]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ca5de33-4fc2-40b9-adfe-b972b17f1c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_label_fuzzy(\"RCT-drug-intervention\", label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6941ae98-24fd-427f-b649-edbb5718fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>accepted_label</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>gpt_predictions_P1_HIERARCHY</th>\n",
       "      <th>gpt_predictions_P2_HIERARCHY</th>\n",
       "      <th>accepted_label_numerical</th>\n",
       "      <th>gpt_predictions_P1_HIERARCHY_numerical</th>\n",
       "      <th>gpt_predictions_P2_HIERARCHY_numerical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32147509</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-systematic-review</td>\n",
       "      <td>Animal-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8800803</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23811310</td>\n",
       "      <td>Non-systematic-review</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36314672</td>\n",
       "      <td>Remaining</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11909745</td>\n",
       "      <td>Remaining</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>Animal-other</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid         accepted_label  multi_label  binary_label  \\\n",
       "0  32147509  Non-systematic-review            1             0   \n",
       "1   8800803  Non-systematic-review            1             0   \n",
       "2  23811310  Non-systematic-review            1             0   \n",
       "3  36314672              Remaining            0             0   \n",
       "4  11909745              Remaining            0             0   \n",
       "\n",
       "  gpt_predictions_P1_HIERARCHY gpt_predictions_P2_HIERARCHY  \\\n",
       "0     Animal-systematic-review     Animal-systematic-review   \n",
       "1                 Animal-other                 Animal-other   \n",
       "2                 Animal-other                 Animal-other   \n",
       "3                 Animal-other                 Animal-other   \n",
       "4                 Animal-other                 Animal-other   \n",
       "\n",
       "   accepted_label_numerical  gpt_predictions_P1_HIERARCHY_numerical  \\\n",
       "0                         1                                       7   \n",
       "1                         1                                       5   \n",
       "2                         1                                       5   \n",
       "3                         0                                       5   \n",
       "4                         0                                       5   \n",
       "\n",
       "   gpt_predictions_P2_HIERARCHY_numerical  \n",
       "0                                       7  \n",
       "1                                       5  \n",
       "2                                       5  \n",
       "3                                       5  \n",
       "4                                       5  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_label_to_numerical(label, label_dict):\n",
    "    # Check if label is a dictionary\n",
    "    if isinstance(label, dict):\n",
    "        print(label)\n",
    "        # Extract the label with the highest score/probability\n",
    "        highest_label = max(normalized_label, key=label.get)\n",
    "        return label_dict.get(highest_label, -1)\n",
    "    else:\n",
    "         # Normalize label\n",
    "        normalized_label = label.replace(',', '').strip().replace(' ', '-')#.strip()\n",
    "        # Directly map string labels to numerical IDs\n",
    "        numerical_label = label_dict.get(label, -1)\n",
    "        # Fuzzy match if no direct mapping possible\n",
    "        if numerical_label == -1:\n",
    "            numerical_label = map_label_fuzzy(normalized_label, label_dict)\n",
    "        # If fuzzy match did not work, check if the label string contains the key 'label' and use it to split the string; keep everything to the right as the potential label\n",
    "        if numerical_label == -1:\n",
    "            if 'label' in label:\n",
    "                label_part = label.split('label')[1]\n",
    "                numerical_label = map_label_fuzzy(label_part, label_dict)\n",
    "        return numerical_label\n",
    "        \n",
    "# Convert accepted labels to numerical\n",
    "df['accepted_label_numerical'] = df['accepted_label'].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "\n",
    "for col in prediction_columns:\n",
    "    df[f'{col}_numerical'] = df[col].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63e63a06-20aa-411e-b65a-a17c0424946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_list_of_lists(lst):\n",
    "    return all(isinstance(sublist, list) for sublist in lst)\n",
    "\n",
    "if is_list_of_lists(prompt_ids_to_eval):\n",
    "    # Flatten the list of lists\n",
    "    prompt_ids_to_eval_flat = [item for sublist in prompt_ids_to_eval for item in sublist]\n",
    "    # Create a string suffix for the CSV file name\n",
    "    csv_file_suffix = '_'.join(prompt_ids_to_eval_flat)\n",
    "else:\n",
    "    csv_file_suffix = '_'.join(prompt_ids_to_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8b62ba05-c24f-46c9-a08f-3894f6aca5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_suffix = 'P2_P1_HIERARCHY_P2_HIERARCHY' #redefine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "41037782-adf1-4715-99d0-c25423b4afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"predictions/{model}_{data_type}_test_outputs_{'_'.join(csv_file_suffix)}_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5fbb-23c4-48b8-be3b-a700d0667854",
   "metadata": {},
   "source": [
    "#### Important: some labels from GPT could not be mapped to a target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64c9a4b8-fe40-4b75-aa4b-47874029ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>accepted_label</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>gpt_predictions_P1_HIERARCHY</th>\n",
       "      <th>gpt_predictions_P2_HIERARCHY</th>\n",
       "      <th>accepted_label_numerical</th>\n",
       "      <th>gpt_predictions_P1_HIERARCHY_numerical</th>\n",
       "      <th>gpt_predictions_P2_HIERARCHY_numerical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pmid, accepted_label, multi_label, binary_label, gpt_predictions_P1_HIERARCHY, gpt_predictions_P2_HIERARCHY, accepted_label_numerical, gpt_predictions_P1_HIERARCHY_numerical, gpt_predictions_P2_HIERARCHY_numerical]\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_with_minus_one = df[(df == -1).any(axis=1)]\n",
    "rows_with_minus_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c211f76-a138-41a2-a16a-97cdaf21a5bd",
   "metadata": {},
   "source": [
    "## Evaluate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1de7219-5c25-4042-b2c1-236f65150fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, csv_file_suffix=None, digits=3):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report_with_ci(y_true, y_pred, numerical_to_label_map=numerical_to_label, round_ndigits = digits)\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report)\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        report_df.set_index('class', inplace=True)\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_{data_type}_P2_{prompt_id}_{eval_type}_with_ci.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "    if not csv_file_suffix:\n",
    "        csv_file_suffix = '_'.join(prompt_ids_to_eval) + \"_\" + eval_type\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_{data_type}_test_per_class_{csv_file_suffix}_{eval_type}_with_ci.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_{data_type}_test_summary_{csv_file_suffix}_{eval_type}_with_ci.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dd952a58-ea4c-4e58-b0a6-fbafc043f336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P2_P1_HIERARCHY_P2_HIERARCHY'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8942176-4e7e-4df1-853f-37a6e1dadfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P1_HIERARCHY', 'P2_HIERARCHY']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids_to_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2c904-2156-4069-9141-398732642c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  P1_HIERARCHY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/statsmodels/stats/proportion.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  q_ = count_a / nobs_a\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/statsmodels/stats/proportion.py:282: RuntimeWarning: divide by zero encountered in divide\n",
      "  denom = 1 + crit2 / nobs_a\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/statsmodels/stats/proportion.py:283: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  center = (q_ + crit2 / (2 * nobs_a)) / denom\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/statsmodels/stats/proportion.py:285: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  q_ * (1.0 - q_) / nobs_a + crit2 / (4.0 * nobs_a**2)\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/confidenceinterval/takahashi_methods.py:83: RuntimeWarning: invalid value encountered in divide\n",
      "  P_i = np.diag(p / total_detected_as_each_category)\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/confidenceinterval/takahashi_methods.py:286: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(delta_method_variance), recall_macro + \\\n",
      "/Users/shirin 1/prodigy/lib/python3.10/site-packages/confidenceinterval/takahashi_methods.py:287: RuntimeWarning: invalid value encountered in sqrt\n",
      "  z * np.sqrt(delta_method_variance)\n"
     ]
    }
   ],
   "source": [
    "target_label_col = 'accepted_label_numerical'\n",
    "eval_type = 'hierarchical'\n",
    "all_reports_df, summary_df = evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, csv_file_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb3349-56a7-438f-81da-172bda7f9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf933f0c-b02b-4356-8b22-045d9fcb1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320f53e-2a57-440e-ac4b-290805dd7d18",
   "metadata": {},
   "source": [
    "## Format Results and Generate LateX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68aab970-6097-4e73-9d8c-2d39904fe9d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'evaluations/gpt-3.5-turbo_enriched_kw_test_summary_P1_P1_HIERARCHY_P2_HIERARCHY_multi_label_with_ci.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m prompt_ids_to_eval \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP1_HIERARCHY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP2_HIERARCHY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m summary_gpt3_raw_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluations/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_test_summary_P1_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids_to_eval\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meval_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_with_ci.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m prompt_ids_to_eval \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP1_HIERARCHY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP2_HIERARCHY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m summary_gpt3_raw_2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluations/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_summary_P2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(prompt_ids_to_eval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_with_ci.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/prodigy/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prodigy/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/prodigy/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prodigy/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/prodigy/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'evaluations/gpt-3.5-turbo_enriched_kw_test_summary_P1_P1_HIERARCHY_P2_HIERARCHY_multi_label_with_ci.csv'"
     ]
    }
   ],
   "source": [
    "# Case of combining separately evaluated prompt files\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prompt_ids_to_eval = [\"P1_HIERARCHY\", \"P2_HIERARCHY\"]\n",
    "summary_gpt3_raw_1 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_P1_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1_HIERARCHY\", \"P2_HIERARCHY\"]\n",
    "summary_gpt3_raw_2 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_P2_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1_HIERARCHY\", \"P2_HIERARCHY\"]\n",
    "summary_gpt3_raw_3 = pd.read_csv(f\"evaluations/{model}_{data_type}_test_summary_P3_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "summary_gpt3_raw = pd.concat([summary_gpt3_raw_1,summary_gpt3_raw_2, summary_gpt3_raw_3]) \n",
    "summary_gpt3_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "9c32739f-f08b-4e9d-8a99-d1aec44e9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_to_concept = {\n",
    "    'P1_HIERARCHY': 'zero-shot',\n",
    "    'P2_HIERARCHY': 'CC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "2f8ffb79-ff55-452c-bd25-a94511c35956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sorting function\n",
    "def custom_sort(prompt):\n",
    "    import re\n",
    "    # Extract numbers from the prompt\n",
    "    numbers = re.findall(r'\\d+', prompt)\n",
    "    if numbers:\n",
    "        # Primary sort by the first number, secondary sort by the full string\n",
    "        return (int(numbers[0]), prompt)\n",
    "    return (float('inf'), prompt)  # Handles cases without numbers\n",
    "    \n",
    "def format_summary_for_latex_report(summary_df, map_prompt_to_concept):\n",
    "    summary_df = summary_df.copy()\n",
    "    \n",
    "    # Concatenating each metric with its CI\n",
    "    summary_df['Precision (CI)'] = summary_df['precision'].astype(str) + ' ' + summary_df['precision CI'].astype(str)\n",
    "    summary_df['Recall (CI)'] = summary_df['recall'].astype(str) + ' ' + summary_df['recall CI'].astype(str)\n",
    "    summary_df['F1-Score (CI)'] = summary_df['f1-score'].astype(str) + ' ' + summary_df['f1-score CI'].astype(str)\n",
    "    summary_df['Accuracy (CI)'] = summary_df['accuracy'].astype(str) + ' ' + summary_df['accuracy CI'].astype(str)\n",
    "    \n",
    "    # Dropping old columns\n",
    "    summary_df.drop(columns=['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI'], inplace=True)\n",
    "    \n",
    "    # Rename 'Prompt ID' to 'Prompt'\n",
    "    summary_df.rename(columns={'Prompt ID': 'Prompt'}, inplace=True)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    summary_df['Concept'] = summary_df['Prompt'].map(map_prompt_to_concept)\n",
    "    \n",
    "    # Rearrange the columns to put 'Concept' after 'Prompt'\n",
    "    summary_df = summary_df[['Prompt', 'Concept', 'Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)']]\n",
    "\n",
    "    summary_df['sort_key'] = summary_df['Prompt'].apply(custom_sort)\n",
    "    summary_df.sort_values('sort_key', inplace=True)\n",
    "    summary_df.drop('sort_key', inplace=True, axis=1)\n",
    "    \n",
    "    return summary_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "5ab2be62-9f1d-4a62-a807-dd1ead802c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Precision (CI)</th>\n",
       "      <th>Recall (CI)</th>\n",
       "      <th>F1-Score (CI)</th>\n",
       "      <th>Accuracy (CI)</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>0.454 (0.351, 0.539)</td>\n",
       "      <td>0.331 (0.292, 0.371)</td>\n",
       "      <td>0.261 (0.22, 0.305)</td>\n",
       "      <td>0.331 (0.292, 0.373)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>0.45 (0.292, 0.652)</td>\n",
       "      <td>0.279 (0.243, 0.318)</td>\n",
       "      <td>0.203 (0.167, 0.242)</td>\n",
       "      <td>0.279 (0.242, 0.316)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3_1</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.584 (0.513, 0.64)</td>\n",
       "      <td>0.453 (0.412, 0.494)</td>\n",
       "      <td>0.43 (0.386, 0.476)</td>\n",
       "      <td>0.453 (0.412, 0.496)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P3_2</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.556 (0.487, 0.614)</td>\n",
       "      <td>0.451 (0.41, 0.494)</td>\n",
       "      <td>0.427 (0.382, 0.472)</td>\n",
       "      <td>0.451 (0.41, 0.494)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P4_1</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.563 (0.498, 0.611)</td>\n",
       "      <td>0.434 (0.393, 0.476)</td>\n",
       "      <td>0.416 (0.372, 0.461)</td>\n",
       "      <td>0.434 (0.393, 0.476)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P4_2</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.506 (0.441, 0.556)</td>\n",
       "      <td>0.414 (0.371, 0.457)</td>\n",
       "      <td>0.392 (0.347, 0.437)</td>\n",
       "      <td>0.414 (0.373, 0.457)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P5</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.608 (0.54, 0.65)</td>\n",
       "      <td>0.511 (0.466, 0.552)</td>\n",
       "      <td>0.498 (0.452, 0.542)</td>\n",
       "      <td>0.511 (0.468, 0.552)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P6</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.591 (0.536, 0.631)</td>\n",
       "      <td>0.541 (0.5, 0.584)</td>\n",
       "      <td>0.532 (0.487, 0.575)</td>\n",
       "      <td>0.541 (0.5, 0.584)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P7</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.467 (0.334, 0.589)</td>\n",
       "      <td>0.305 (0.266, 0.345)</td>\n",
       "      <td>0.229 (0.19, 0.269)</td>\n",
       "      <td>0.305 (0.268, 0.345)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P9</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.542 (0.453, 0.616)</td>\n",
       "      <td>0.41 (0.371, 0.453)</td>\n",
       "      <td>0.375 (0.332, 0.42)</td>\n",
       "      <td>0.41 (0.369, 0.453)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P9_1</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.521 (0.444, 0.589)</td>\n",
       "      <td>0.406 (0.365, 0.448)</td>\n",
       "      <td>0.375 (0.332, 0.42)</td>\n",
       "      <td>0.406 (0.365, 0.449)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P10</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.486 (0.393, 0.565)</td>\n",
       "      <td>0.354 (0.315, 0.395)</td>\n",
       "      <td>0.291 (0.248, 0.335)</td>\n",
       "      <td>0.354 (0.315, 0.395)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P11</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.584 (0.511, 0.64)</td>\n",
       "      <td>0.442 (0.399, 0.485)</td>\n",
       "      <td>0.417 (0.372, 0.463)</td>\n",
       "      <td>0.442 (0.399, 0.485)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P11_1</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.589 (0.517, 0.644)</td>\n",
       "      <td>0.451 (0.41, 0.496)</td>\n",
       "      <td>0.428 (0.383, 0.473)</td>\n",
       "      <td>0.451 (0.41, 0.494)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P11_2</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.591 (0.525, 0.644)</td>\n",
       "      <td>0.463 (0.421, 0.506)</td>\n",
       "      <td>0.443 (0.397, 0.487)</td>\n",
       "      <td>0.463 (0.419, 0.506)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P11_3</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.636 (0.577, 0.685)</td>\n",
       "      <td>0.504 (0.461, 0.545)</td>\n",
       "      <td>0.488 (0.444, 0.534)</td>\n",
       "      <td>0.504 (0.463, 0.547)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P11_4</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.601 (0.543, 0.644)</td>\n",
       "      <td>0.528 (0.487, 0.571)</td>\n",
       "      <td>0.518 (0.474, 0.564)</td>\n",
       "      <td>0.528 (0.485, 0.571)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P11_5</td>\n",
       "      <td>CoT + CC</td>\n",
       "      <td>0.563 (0.504, 0.607)</td>\n",
       "      <td>0.425 (0.382, 0.466)</td>\n",
       "      <td>0.404 (0.359, 0.45)</td>\n",
       "      <td>0.425 (0.384, 0.468)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P12</td>\n",
       "      <td>2 CoT + CC</td>\n",
       "      <td>0.539 (0.456, 0.604)</td>\n",
       "      <td>0.403 (0.361, 0.444)</td>\n",
       "      <td>0.371 (0.328, 0.418)</td>\n",
       "      <td>0.403 (0.361, 0.446)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P12_1</td>\n",
       "      <td>2 CoT + CC</td>\n",
       "      <td>0.588 (0.506, 0.652)</td>\n",
       "      <td>0.459 (0.418, 0.502)</td>\n",
       "      <td>0.422 (0.375, 0.466)</td>\n",
       "      <td>0.459 (0.418, 0.502)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P12_2</td>\n",
       "      <td>2 CoT + CC</td>\n",
       "      <td>0.605 (0.548, 0.651)</td>\n",
       "      <td>0.464 (0.425, 0.507)</td>\n",
       "      <td>0.445 (0.398, 0.491)</td>\n",
       "      <td>0.464 (0.421, 0.506)</td>\n",
       "      <td>gpt-3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt     Concept        Precision (CI)           Recall (CI)  \\\n",
       "0      P1   zero-shot  0.454 (0.351, 0.539)  0.331 (0.292, 0.371)   \n",
       "1      P2   zero-shot   0.45 (0.292, 0.652)  0.279 (0.243, 0.318)   \n",
       "2    P3_1          CC   0.584 (0.513, 0.64)  0.453 (0.412, 0.494)   \n",
       "3    P3_2          CC  0.556 (0.487, 0.614)   0.451 (0.41, 0.494)   \n",
       "6    P4_1          CC  0.563 (0.498, 0.611)  0.434 (0.393, 0.476)   \n",
       "7    P4_2          CC  0.506 (0.441, 0.556)  0.414 (0.371, 0.457)   \n",
       "8      P5          CC    0.608 (0.54, 0.65)  0.511 (0.466, 0.552)   \n",
       "9      P6          CC  0.591 (0.536, 0.631)    0.541 (0.5, 0.584)   \n",
       "10     P7         CoT  0.467 (0.334, 0.589)  0.305 (0.266, 0.345)   \n",
       "11     P9    CoT + CC  0.542 (0.453, 0.616)   0.41 (0.371, 0.453)   \n",
       "12   P9_1    CoT + CC  0.521 (0.444, 0.589)  0.406 (0.365, 0.448)   \n",
       "13    P10         CoT  0.486 (0.393, 0.565)  0.354 (0.315, 0.395)   \n",
       "14    P11    CoT + CC   0.584 (0.511, 0.64)  0.442 (0.399, 0.485)   \n",
       "15  P11_1    CoT + CC  0.589 (0.517, 0.644)   0.451 (0.41, 0.496)   \n",
       "16  P11_2    CoT + CC  0.591 (0.525, 0.644)  0.463 (0.421, 0.506)   \n",
       "17  P11_3    CoT + CC  0.636 (0.577, 0.685)  0.504 (0.461, 0.545)   \n",
       "18  P11_4    CoT + CC  0.601 (0.543, 0.644)  0.528 (0.487, 0.571)   \n",
       "19  P11_5    CoT + CC  0.563 (0.504, 0.607)  0.425 (0.382, 0.466)   \n",
       "20    P12  2 CoT + CC  0.539 (0.456, 0.604)  0.403 (0.361, 0.444)   \n",
       "21  P12_1  2 CoT + CC  0.588 (0.506, 0.652)  0.459 (0.418, 0.502)   \n",
       "22  P12_2  2 CoT + CC  0.605 (0.548, 0.651)  0.464 (0.425, 0.507)   \n",
       "\n",
       "           F1-Score (CI)         Accuracy (CI)    Model  \n",
       "0    0.261 (0.22, 0.305)  0.331 (0.292, 0.373)  gpt-3.5  \n",
       "1   0.203 (0.167, 0.242)  0.279 (0.242, 0.316)  gpt-3.5  \n",
       "2    0.43 (0.386, 0.476)  0.453 (0.412, 0.496)  gpt-3.5  \n",
       "3   0.427 (0.382, 0.472)   0.451 (0.41, 0.494)  gpt-3.5  \n",
       "6   0.416 (0.372, 0.461)  0.434 (0.393, 0.476)  gpt-3.5  \n",
       "7   0.392 (0.347, 0.437)  0.414 (0.373, 0.457)  gpt-3.5  \n",
       "8   0.498 (0.452, 0.542)  0.511 (0.468, 0.552)  gpt-3.5  \n",
       "9   0.532 (0.487, 0.575)    0.541 (0.5, 0.584)  gpt-3.5  \n",
       "10   0.229 (0.19, 0.269)  0.305 (0.268, 0.345)  gpt-3.5  \n",
       "11   0.375 (0.332, 0.42)   0.41 (0.369, 0.453)  gpt-3.5  \n",
       "12   0.375 (0.332, 0.42)  0.406 (0.365, 0.449)  gpt-3.5  \n",
       "13  0.291 (0.248, 0.335)  0.354 (0.315, 0.395)  gpt-3.5  \n",
       "14  0.417 (0.372, 0.463)  0.442 (0.399, 0.485)  gpt-3.5  \n",
       "15  0.428 (0.383, 0.473)   0.451 (0.41, 0.494)  gpt-3.5  \n",
       "16  0.443 (0.397, 0.487)  0.463 (0.419, 0.506)  gpt-3.5  \n",
       "17  0.488 (0.444, 0.534)  0.504 (0.463, 0.547)  gpt-3.5  \n",
       "18  0.518 (0.474, 0.564)  0.528 (0.485, 0.571)  gpt-3.5  \n",
       "19   0.404 (0.359, 0.45)  0.425 (0.384, 0.468)  gpt-3.5  \n",
       "20  0.371 (0.328, 0.418)  0.403 (0.361, 0.446)  gpt-3.5  \n",
       "21  0.422 (0.375, 0.466)  0.459 (0.418, 0.502)  gpt-3.5  \n",
       "22  0.445 (0.398, 0.491)  0.464 (0.421, 0.506)  gpt-3.5  "
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_gpt3 = format_summary_for_latex_report(summary_gpt3_raw, map_prompt_to_concept)\n",
    "summary_gpt3['Model'] = 'gpt-3.5'\n",
    "summary_gpt3 = summary_gpt3.dropna(subset=['Concept'])\n",
    "summary_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "b511106a-7789-4bdb-8064-fd01214fb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Prompt & Concept & Precision (CI) & Recall (CI) & F1-Score (CI) \\\\\n",
      "\\midrule\n",
      "P1 & zero-shot & 0.454 (0.351, 0.539) & 0.331 (0.292, 0.371) & 0.261 (0.22, 0.305) \\\\\n",
      "P2 & zero-shot & 0.45 (0.292, 0.652) & 0.279 (0.243, 0.318) & 0.203 (0.167, 0.242) \\\\\n",
      "P3\\_1 & CC & 0.584 (0.513, 0.64) & 0.453 (0.412, 0.494) & 0.43 (0.386, 0.476) \\\\\n",
      "P3\\_2 & CC & 0.556 (0.487, 0.614) & 0.451 (0.41, 0.494) & 0.427 (0.382, 0.472) \\\\\n",
      "P4\\_1 & CC & 0.563 (0.498, 0.611) & 0.434 (0.393, 0.476) & 0.416 (0.372, 0.461) \\\\\n",
      "P4\\_2 & CC & 0.506 (0.441, 0.556) & 0.414 (0.371, 0.457) & 0.392 (0.347, 0.437) \\\\\n",
      "P5 & CC & 0.608 (0.54, 0.65) & 0.511 (0.466, 0.552) & 0.498 (0.452, 0.542) \\\\\n",
      "P6 & CC & 0.591 (0.536, 0.631) & 0.541 (0.5, 0.584) & 0.532 (0.487, 0.575) \\\\\n",
      "P7 & CoT & 0.467 (0.334, 0.589) & 0.305 (0.266, 0.345) & 0.229 (0.19, 0.269) \\\\\n",
      "P9 & CoT + CC & 0.542 (0.453, 0.616) & 0.41 (0.371, 0.453) & 0.375 (0.332, 0.42) \\\\\n",
      "P9\\_1 & CoT + CC & 0.521 (0.444, 0.589) & 0.406 (0.365, 0.448) & 0.375 (0.332, 0.42) \\\\\n",
      "P10 & CoT & 0.486 (0.393, 0.565) & 0.354 (0.315, 0.395) & 0.291 (0.248, 0.335) \\\\\n",
      "P11 & CoT + CC & 0.584 (0.511, 0.64) & 0.442 (0.399, 0.485) & 0.417 (0.372, 0.463) \\\\\n",
      "P11\\_1 & CoT + CC & 0.589 (0.517, 0.644) & 0.451 (0.41, 0.496) & 0.428 (0.383, 0.473) \\\\\n",
      "P11\\_2 & CoT + CC & 0.591 (0.525, 0.644) & 0.463 (0.421, 0.506) & 0.443 (0.397, 0.487) \\\\\n",
      "P11\\_3 & CoT + CC & 0.636 (0.577, 0.685) & 0.504 (0.461, 0.545) & 0.488 (0.444, 0.534) \\\\\n",
      "P11\\_4 & CoT + CC & 0.601 (0.543, 0.644) & 0.528 (0.487, 0.571) & 0.518 (0.474, 0.564) \\\\\n",
      "P11\\_5 & CoT + CC & 0.563 (0.504, 0.607) & 0.425 (0.382, 0.466) & 0.404 (0.359, 0.45) \\\\\n",
      "P12 & 2 CoT + CC & 0.539 (0.456, 0.604) & 0.403 (0.361, 0.444) & 0.371 (0.328, 0.418) \\\\\n",
      "P12\\_1 & 2 CoT + CC & 0.588 (0.506, 0.652) & 0.459 (0.418, 0.502) & 0.422 (0.375, 0.466) \\\\\n",
      "P12\\_2 & 2 CoT + CC & 0.605 (0.548, 0.651) & 0.464 (0.425, 0.507) & 0.445 (0.398, 0.491) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_gpt3.drop(columns=['Model', 'Accuracy (CI)'], inplace=True)\n",
    "\n",
    "print(summary_gpt3.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodigy",
   "language": "python",
   "name": "prodigy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
