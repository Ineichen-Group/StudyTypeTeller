{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa9dbf2-fa59-4bd7-8f47-9662c26fb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import pairwise\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb91fd8e-52bf-4dc6-bf49-c7021ae94357",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_data_path = Path(\"./data/embeddings/\")\n",
    "input_data_path = Path(\"./data/data_splits_stratified/6-2-2_all_classes_enriched_with_kw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b127ebd-a9f3-4a6c-b768-3bb59b524a7d",
   "metadata": {},
   "source": [
    "## Load Embeddings and Calculate Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71d418-53d7-4a0a-9b45-b0f87b4ce97f",
   "metadata": {},
   "source": [
    "### Load \n",
    "-> each row represents the text from one sample embedded into a 768-demnsional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe85add-9d56-4343-9213-d815c2167b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/embeddings/embeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_train_ds.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_train_ds.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m embeddings_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m      5\u001b[0m     embeddings_data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_test_ds.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/embeddings/embeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_train_ds.npy'"
     ]
    }
   ],
   "source": [
    "embeddings_train = np.load(\n",
    "    embeddings_data_path / \"embeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_train_ds.npy\"\n",
    ")\n",
    "embeddings_test = np.load(\n",
    "    embeddings_data_path / \"embeddings_microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_test_ds.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566771e-fd87-412d-a2f0-46e624f3df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36822e6a-fdbc-4bb3-bfa8-66462c6dab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35fa86-de31-4708-a0f4-ac95ba1831be",
   "metadata": {},
   "source": [
    "### Compute similarity between test dataset elements to all train elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059aad4-0a35-4e08-b03b-6c79660f10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = pairwise_distances(embeddings_test, embeddings_train, metric='sqeuclidean')\n",
    "dist_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878fa42-0d3c-44ee-a2f1-e0d2b7edaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_cosine = pairwise.cosine_similarity(X=embeddings_test, Y=embeddings_train)\n",
    "dist_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069776ed-df11-4df0-88c1-22551e65770d",
   "metadata": {},
   "source": [
    "The matrix has the dimensions 404 (test data elements) x 1191 (train data elements). Each of the 1191 values per test row is the similarity score between the test element to the train elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1450ee-4ad6-4240-b539-e3704cdd9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18626bf4-dd42-4931-8add-bcb419b4021d",
   "metadata": {},
   "source": [
    "### Find closest neighbours from the train dataset to each test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8c0be-e671-4192-87f2-fdff8d2855d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(-torch.from_numpy(dist_matrix_cosine), k=3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553bc67-1fbc-4ec7-b6b6-37b70ac78fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4190b-4ee2-4971-a9ce-4c63ce49f68f",
   "metadata": {},
   "source": [
    "The indices have the dimension 404 (test data elements) x 3 (top k=3 closest train data elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76c1a3-c30c-4661-8c93-a3e40d882c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb714c0-8c4d-4b80-8556-f0dd42276ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(input_data_path/ 'train.csv')\n",
    "df_test = pd.read_csv(input_data_path/ 'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7080ab-0eb0-425f-bf03-b5471192f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['accepted_label'] == 'In-vitro-study']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e246dd-50e5-4e20-9122-a5d377851a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc87b3-9d46-45ce-b76e-7fc4a6f653b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d817841-de89-44bb-8068-98341a39bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[1032]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc00f5-f841-4951-8d39-c733df807620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('accepted_label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee3048-c2c3-42c2-b411-bea8fcf78406",
   "metadata": {},
   "source": [
    "## Init OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4414af75-5dc8-44d7-b346-10001db3a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pass(file_path, key_to_find):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"=\")\n",
    "            if len(parts) == 2 and parts[0] == key_to_find:\n",
    "                found_password = parts[1]\n",
    "                break\n",
    "    if found_password:\n",
    "        print(\"Found password.\")\n",
    "        return found_password\n",
    "    else:\n",
    "        print(\"Password not found for key:\", key_to_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6554c-112c-45a6-9b54-0c33ee01597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = load_pass(\"./credentials.txt\", \"OPENAI\")\n",
    "client = OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6cd2e-a6b2-46ef-9165-258325a719d8",
   "metadata": {},
   "source": [
    "## Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7b7b4-6e05-4dfe-a7fc-3f7460f19f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_journal_title_abstract'] = '<journal>' + df_train['journal_name'] + '</journal>' + \\\n",
    "                                         '<title>' + df_train['title'] + '</title>' + \\\n",
    "                                         '<abstract>' + df_train['abstract'] + '</abstract>'\n",
    "df_test['input_journal_title_abstract'] = '<journal>' + df_test['journal_name'] + '</journal>' + \\\n",
    "                                         '<title>' + df_test['title'] + '</title>' + \\\n",
    "                                         '<abstract>' + df_test['abstract'] + '</abstract>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528c2fb-58e0-48dc-92a2-595ce826da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d427668-7f0c-4417-93e1-edd4781e8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(df_train, df_test, test_index, example_indices):\n",
    "    # Start the prompt with a task description (optional)\n",
    "    prompt = \"Classify this text, choosing one of these labels: Clinical-study-protocol, Human-systematic-review, Non-systematic-review, Human-RCT-non-drug-intervention, Human-RCT-drug-intervention, Human-RCT-non-intervention, Human-case-report, Human-non-RCT-non-drug-intervention, Human-non-RCT-drug-intervention, Animal-systematic-review, Animal-drug-intervention, Animal-non-drug-intervention, Animal-other, In-vitro-study, Remaining. Respond in json format with the key: gpt_label.\\n\\n\"\n",
    "    \n",
    "    # Add examples from df_train\n",
    "    for idx in example_indices:\n",
    "        example_text = df_train.loc[idx, 'input_journal_title_abstract']\n",
    "        example_label = df_train.loc[idx, 'accepted_label']\n",
    "        prompt += f\"Text: \\\"{example_text}\\\"\\nCategory: {example_label}\\n\\n\"\n",
    "    \n",
    "    # Add the test text needing classification\n",
    "    test_text = df_test.loc[test_index, 'input_journal_title_abstract']\n",
    "    prompt += f\"Text: \\\"{test_text}\\\"\\nCategory: \"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581a984-8a2d-4f0d-9233-b89e557c56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_i = 0\n",
    "example_prompt = create_prompt(df_train, df_test, example_i, indices[example_i].tolist())\n",
    "#example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e2a71-2ebc-4b3b-a9a9-caaf6430d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_prompt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba3cd3-cf8c-4002-8463-7ab81a06c9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b5eec-f852-4c84-b6ef-42e2bc0633b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEFAULT_TEMPERATURE = 0\n",
    "\n",
    "def create_prompt(df_train, example_indices, input_raw_text):\n",
    "    prompt = \"Classify this text, choosing one of these labels: Clinical-study-protocol, Human-systematic-review, Non-systematic-review, Human-RCT-non-drug-intervention, Human-RCT-drug-intervention, Human-RCT-non-intervention, Human-case-report, Human-non-RCT-non-drug-intervention, Human-non-RCT-drug-intervention, Animal-systematic-review, Animal-drug-intervention, Animal-non-drug-intervention, Animal-other, In-vitro-study, Remaining. Respond in json format with the key: gpt_label.\\n\\n\"\n",
    "    for idx in example_indices:\n",
    "        example_text = df_train.loc[idx, 'input_journal_title_abstract']\n",
    "        example_label = df_train.loc[idx, 'accepted_label']\n",
    "        prompt += f\"Text: \\\"{example_text}\\\"\\nCategory: {example_label}\\n\\n\"\n",
    "    prompt += f\"Text: \\\"{input_raw_text}\\\"\\nCategory: \"\n",
    "    return prompt\n",
    "\n",
    "def query_gpt(df_train, input_raw_text, example_indices, gpt_model=\"gpt-3.5-turbo\", temperature=DEFAULT_TEMPERATURE, max_retries=5, retry_delay=3):\n",
    "    prompt_text = create_prompt(df_train, example_indices, input_raw_text)\n",
    "    system_msg = f\"You are an expert assistant specialized in text classification of PubMed abstracts.\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        print(\"Trying to call OpenAI API...\")\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=gpt_model,  \n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": prompt_text}\n",
    "                ]\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI API returned an error: {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "            retries += 1\n",
    "\n",
    "    raise RuntimeError(\"Max retries reached. Unable to complete the API call.\")\n",
    "\n",
    "def apply_gpt_with_progress(df_train, test_data_series, example_indices_tensor=None, num_samples=3, use_random=False, model=\"gpt-3.5-turbo\"):\n",
    "    results = []\n",
    "    total_items = len(test_data_series)\n",
    "    with tqdm(total=total_items, desc=\"Processing dataset\") as pbar:\n",
    "        for i, text in enumerate(test_data_series):\n",
    "            if use_random:\n",
    "                example_indices = random.sample(range(len(df_train)), num_samples)\n",
    "            else:\n",
    "                example_indices = example_indices_tensor[i].tolist()\n",
    "            print(\"Retrieved in-context learning examples with idx: \", example_indices)\n",
    "            result = query_gpt(df_train, text, example_indices, model)\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e607caa-60c9-4416-97da-f2d696e76215",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gpt(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48530511-5390-498a-8b76-e4e2cef2f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df_test[f'gpt_predictions_in_context'] = apply_gpt_with_progress(df_train, df_test['input_journal_title_abstract'], indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc995dd-0672-4e38-855d-f6951a52463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[f'gpt_predictions_in_context_random'] = apply_gpt_with_progress(df_train, df_test['input_journal_title_abstract'], use_random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c5dc0-f6e8-4254-a64d-e61e44916dd0",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b214fd7-4b5e-401f-be72-3e61f3335eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_to_eval = df_test.copy()\n",
    "df_test_to_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bb4b1-2f9c-4fcd-be0a-e7b78afb479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids_to_test = [\"in_context\", \"in_context_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c195170-c8c0-4cef-b6a7-d1911f6dabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Human-systematic-review\", \"Human-RCT-drug-intervention\", \"Human-RCT-non-drug-intervention\", \"Human-RCT-non-intervention\", \"Human-case-report\", \"Human-non-RCT-drug-intervention\", \"Human-non-RCT-non-drug-intervention\", \"Animal-systematic-review\", \"Animal-drug-intervention\", \"Animal-non-drug-intervention\", \"Animal-other\", \"Non-systematic-review\", \"In-vitro-study\", \"Clinical-study-protocol\", \"Remaining\"]\n",
    "\n",
    "label_to_numerical = {label: i for i, label in enumerate(labels)}\n",
    "label_to_numerical[\"label missing\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c402b-ecd8-4968-903b-9304fc1052f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numerical(label):\n",
    "    # Check if label is a dictionary\n",
    "    if isinstance(label, dict):\n",
    "        # Extract the label with the highest score/probability\n",
    "        highest_label = max(label, key=label.get)\n",
    "        return label_to_numerical.get(highest_label, -1)\n",
    "    else:\n",
    "        # Directly map string labels to numerical IDs\n",
    "        return label_to_numerical.get(label, -1)\n",
    "        \n",
    "# Convert accepted labels to numerical\n",
    "df_test_to_eval['accepted_label_numerical'] = df_test_to_eval['accepted_label'].apply(lambda x: label_to_numerical.get(x, -1))\n",
    "\n",
    "\n",
    "# Initialize a list to hold DataFrame for each report and summary statistics\n",
    "report_dfs = []\n",
    "summary_stats = []\n",
    "\n",
    "# Iterate over each GPT prediction column\n",
    "for prompt_id in prompt_ids_to_test:\n",
    "    print(\"Evaluating \", prompt_id)\n",
    "    prediction_col = f'gpt_predictions_{prompt_id}_clean'\n",
    "\n",
    "    df_test_to_eval[prediction_col] = df_test_to_eval[f'gpt_predictions_{prompt_id}'].apply(\n",
    "            lambda x: json.loads(x)['gpt_label'] if isinstance(x, str) and 'gpt_label' in json.loads(x) else x\n",
    "        )\n",
    "    \n",
    "    # Map GPT predictions to numerical values\n",
    "    df_test_to_eval[f'{prediction_col}_numerical'] = df_test_to_eval[prediction_col].apply(map_label_to_numerical)\n",
    "\n",
    "    # Extract arrays for evaluation\n",
    "    y_true = df_test_to_eval['accepted_label_numerical'].values\n",
    "    y_pred = df_test_to_eval[f'{prediction_col}_numerical'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0, labels=range(len(labels)), target_names=labels)\n",
    "    \n",
    "    # Create DataFrame from report\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df['Prompt ID'] = prompt_id  # Add column to indicate the prompt ID\n",
    "    report_dfs.append(report_df)\n",
    "    \n",
    "    # Extract summary statistics (average precision, recall, F1)\n",
    "    summary = report_df.loc['weighted avg', ['precision', 'recall', 'f1-score']].to_dict()\n",
    "    summary['Prompt ID'] = prompt_id\n",
    "    summary_stats.append(summary)\n",
    "\n",
    "    # Plotting confusion matrix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "    plt.title(f'Confusion Matrix for Model {model} and ICL-Prompt {prompt_id}', fontsize=14)\n",
    "    plt.xlabel('Predicted Labels', fontsize=13)\n",
    "    plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "    # Add an inset with label mapping\n",
    "    textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/confusion_matrix_{model}_enriched_kw_test_{prompt_id}_{eval_type}_ICL.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7212c04-38e3-457e-a1b4-683a2c54eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575268f0-d42b-45f5-9727-09369d76389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b9b81-4947-4de0-87ea-df06f3b4270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3424e5-fdb1-48dc-b468-7a77224bf664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
