{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e861e18f-284f-4bd3-a15d-c2d83f82f090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e83a32-ca98-485b-b374-1ffb753cf5fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'confidenceinterval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfidenceinterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report_with_ci\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfidenceinterval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbootstrap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bootstrap_ci\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'confidenceinterval'"
     ]
    }
   ],
   "source": [
    "from confidenceinterval import classification_report_with_ci\n",
    "from confidenceinterval.bootstrap import bootstrap_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23737756-37bc-4ece-8f25-d613023365a1",
   "metadata": {},
   "source": [
    "# Evaluate Multi-Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1f875-9e49-4012-ad47-80c82660e8c5",
   "metadata": {},
   "source": [
    "## Load file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac99f9a-f33f-444c-b35f-75e9de6f4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids_to_eval = [\"P1\"] #, \"P2\", \"P3\", \"P4\"\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451a2a7-aa01-4423-b816-d628e324ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of prediction columns based on the prompt IDs\n",
    "prediction_columns = [f'gpt_predictions_{prompt_id}' for prompt_id in prompt_ids_to_eval]\n",
    "# Specify the basic columns to include in the DataFrame\n",
    "basic_columns = ['pmid', 'accepted_label', 'multi_label', 'binary_label']\n",
    "# Combine basic columns with the dynamically generated prediction columns\n",
    "columns_to_read = basic_columns + prediction_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae494e-362b-4ad9-bb16-e1b0d4f6c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"predictions/{model}_enriched_kw_test_outputs_{'_'.join(prompt_ids_to_eval)}.csv\")[columns_to_read]\n",
    "#df = pd.read_csv(f\"predictions/{model}_enriched_test_outputs_{'_'.join(prompt_ids_to_eval)}.csv\")[columns_to_read]\n",
    "#df = pd.read_csv(f\"predictions/{model}_test_outputs_{'_'.join(prompt_ids_to_eval)}.csv\")[columns_to_read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d208e-7607-4467-8bee-642018fea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421e880-ddc3-4dc5-be01-72f97a9ee980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae89110-d694-4650-8b3c-81e46a8b6471",
   "metadata": {},
   "source": [
    "## Map predictions to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860b2b-86a3-4fc3-bd0e-1556a2ac89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_numerical = {\n",
    "    'Remaining': 0,\n",
    "    'Non-systematic-review': 1,\n",
    "    'Human-non-RCT-non-drug-intervention': 2,\n",
    "    'Human-non-RCT-drug-intervention': 3,\n",
    "    'Human-case-report': 4,\n",
    "    'Animal-other': 5,\n",
    "    'Animal-drug-intervention': 6,\n",
    "    'Human-systematic-review': 7,\n",
    "    'In-vitro-study': 8,\n",
    "    'Human-RCT-non-drug-intervention': 9,\n",
    "    'Animal-non-drug-intervention': 10,\n",
    "    'Human-RCT-drug-intervention': 11,\n",
    "    'Clinical-study-protocol': 12,\n",
    "    'Human-RCT-non-intervention': 13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf7b15-4a76-46f6-a687-19952dd8c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_to_label = {v: f\"{v} - {k}\" for k, v in label_to_numerical.items()}\n",
    "numerical_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657b963-b35c-4d0a-9b5d-aa8ddeb267d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def map_label_fuzzy(label, label_dict):\n",
    "    #label = label.lower().replace('-', '').replace('_', ' ').strip()\n",
    "    best_match = difflib.get_close_matches(label, label_dict.keys(), n=1, cutoff=0.6)\n",
    "    if best_match:\n",
    "        #print(f'{label} matched to {best_match[0]}')\n",
    "        return label_dict[best_match[0]]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5de33-4fc2-40b9-adfe-b972b17f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label_fuzzy(\"RCT-drug-intervention\", label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ae98-24fd-427f-b649-edbb5718fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numerical(label, label_dict):\n",
    "    # Check if label is a dictionary\n",
    "    if isinstance(label, dict):\n",
    "        # Extract the label with the highest score/probability\n",
    "        highest_label = max(normalized_label, key=label.get)\n",
    "        return label_dict.get(highest_label, -1)\n",
    "    else:\n",
    "         # Normalize label\n",
    "        normalized_label = label.replace(',', '').strip().replace(' ', '-')#.strip()\n",
    "        # Directly map string labels to numerical IDs\n",
    "        numerical_label = label_dict.get(label, -1)\n",
    "        # Fuzzy match if no direct mapping possible\n",
    "        if numerical_label == -1:\n",
    "            numerical_label = map_label_fuzzy(normalized_label, label_dict)\n",
    "        # If fuzzy match did not work, check if the label string contains the key 'label' and use it to split the string; keep everything to the right as the potential label\n",
    "        if numerical_label == -1:\n",
    "            if 'label' in label:\n",
    "                label_part = label.split('label')[1]\n",
    "                numerical_label = map_label_fuzzy(label_part, label_dict)\n",
    "        return numerical_label\n",
    "        \n",
    "# Convert accepted labels to numerical\n",
    "df['accepted_label_numerical'] = df['accepted_label'].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "\n",
    "for col in prediction_columns:\n",
    "    df[f'{col}_numerical'] = df[col].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b5567-d404-49d6-85a5-bf717a3910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"predictions/{model}_enriched_kw_test_test_outputs_{'_'.join(prompt_ids_to_eval)}_structured.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5fbb-23c4-48b8-be3b-a700d0667854",
   "metadata": {},
   "source": [
    "#### Important: some labels from GPT could not be mapped to a target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a4b8-fe40-4b75-aa4b-47874029ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_minus_one = df[(df == -1).any(axis=1)]\n",
    "rows_with_minus_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c211f76-a138-41a2-a16a-97cdaf21a5bd",
   "metadata": {},
   "source": [
    "## Evaluate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f897e-17c7-4005-a2bd-891f13baa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, digits=3):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, numerical_to_label_map=numerical_to_label, round_ndigits = digits)\n",
    "\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report)\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        report_df.set_index('class', inplace=True)\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_enriched_kw_test_{prompt_id}_{eval_type}_with_ci.pdf')  # Save to PDF\n",
    "        #plt.savefig(f'plots/confusion_matrix_{model}_enriched_test_{prompt_id}_{eval_type}.pdf')  # Save to PDF\n",
    "        #plt.savefig(f'plots/confusion_matrix_{model}_test_{prompt_id}_{eval_type}.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_enriched_kw_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\")\n",
    "    #all_reports_df.to_csv(f\"evaluations/{model}_enriched_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #summary_df.to_csv(f\"evaluations/{model}_enriched_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #all_reports_df.to_csv(f\"evaluations/{model}_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #summary_df.to_csv(f\"evaluations/{model}_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2c904-2156-4069-9141-398732642c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_col = 'accepted_label_numerical'\n",
    "eval_type = 'multi_label'\n",
    "all_reports_df, summary_df = evaluate_predictions(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, digits=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d603cc-2300-4562-8e8d-cde88c2baabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6172fa-7032-47ec-b183-4a40a23f5649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_reports_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
