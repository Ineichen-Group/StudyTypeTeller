{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e861e18f-284f-4bd3-a15d-c2d83f82f090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cf70b9-0b24-44b1-9f09-b4537aa968e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'confidenceinterval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfidenceinterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report_with_ci\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfidenceinterval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbootstrap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bootstrap_ci\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'confidenceinterval'"
     ]
    }
   ],
   "source": [
    "from confidenceinterval import classification_report_with_ci\n",
    "from confidenceinterval.bootstrap import bootstrap_ci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f306c2-15f0-4a43-a616-0b1b24d2cce5",
   "metadata": {},
   "source": [
    "#### small demo of CI lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67195c70-06b4-43b4-8a29-9b4c4c6125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 2, 2, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1]\n",
    "y_pred = [0, 1, 0, 0, 2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1]\n",
    "# You can specify a random generator for reproducability, or pass None\n",
    "random_generator = np.random.default_rng()\n",
    "bootstrap_ci(y_true=y_true,\n",
    "             y_pred=y_pred,\n",
    "             metric=sklearn.metrics.accuracy_score, # change to any metrics\n",
    "             metric_average='n.a.', # if multi-class you can specify the average method, i.e. ‘micro’, ‘macro’, ‘samples’, ‘weighted’\n",
    "             confidence_level=0.95,\n",
    "             n_resamples=9999,\n",
    "             method='bootstrap_bca',\n",
    "             random_state=random_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd8f2d-31e0-4f3b-bdd4-29e9b4778ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_with_ci(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23737756-37bc-4ece-8f25-d613023365a1",
   "metadata": {},
   "source": [
    "# Evaluate Multi-Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1f875-9e49-4012-ad47-80c82660e8c5",
   "metadata": {},
   "source": [
    "## Load file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac99f9a-f33f-444c-b35f-75e9de6f4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids_to_eval = [\"P7\"] #[\"P1\", \"P4_1\", \"P5\"] # [\"P6\", \"P7\", \"P11_3\", \"P11_4\"]\n",
    "model = \"gpt-3.5-turbo\" #\"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451a2a7-aa01-4423-b816-d628e324ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of prediction columns based on the prompt IDs\n",
    "prediction_columns = [f'gpt_predictions_{prompt_id}' for prompt_id in prompt_ids_to_eval]\n",
    "# Specify the basic columns to include in the DataFrame\n",
    "basic_columns = ['pmid', 'accepted_label', 'multi_label', 'binary_label']\n",
    "# Combine basic columns with the dynamically generated prediction columns\n",
    "columns_to_read = basic_columns + prediction_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae494e-362b-4ad9-bb16-e1b0d4f6c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt-4-turbo-preview_enriched_kw_test_outputs_P6_P7_P11_3_P11_4.csv \n",
    "df = pd.read_csv(f\"predictions/{model}_enriched_kw_test_outputs_{'_'.join(prompt_ids_to_eval)}.csv\")[columns_to_read]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d208e-7607-4467-8bee-642018fea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421e880-ddc3-4dc5-be01-72f97a9ee980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae89110-d694-4650-8b3c-81e46a8b6471",
   "metadata": {},
   "source": [
    "## Map predictions to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860b2b-86a3-4fc3-bd0e-1556a2ac89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_numerical = {\n",
    "    'Remaining': 0,\n",
    "    'Non-systematic-review': 1,\n",
    "    'Human-non-RCT-non-drug-intervention': 2,\n",
    "    'Human-non-RCT-drug-intervention': 3,\n",
    "    'Human-case-report': 4,\n",
    "    'Animal-other': 5,\n",
    "    'Animal-drug-intervention': 6,\n",
    "    'Human-systematic-review': 7,\n",
    "    'In-vitro-study': 8,\n",
    "    'Human-RCT-non-drug-intervention': 9,\n",
    "    'Animal-non-drug-intervention': 10,\n",
    "    'Human-RCT-drug-intervention': 11,\n",
    "    'Clinical-study-protocol': 12,\n",
    "    'Human-RCT-non-intervention': 13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf7b15-4a76-46f6-a687-19952dd8c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_to_label = {v: f\"{k}\" for k, v in label_to_numerical.items()}\n",
    "numerical_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657b963-b35c-4d0a-9b5d-aa8ddeb267d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def map_label_fuzzy(label, label_dict):\n",
    "    #label = label.lower().replace('-', '').replace('_', ' ').strip()\n",
    "    best_match = difflib.get_close_matches(label, label_dict.keys(), n=1, cutoff=0.6)\n",
    "    if best_match:\n",
    "        #print(f'{label} matched to {best_match[0]}')\n",
    "        return label_dict[best_match[0]]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5de33-4fc2-40b9-adfe-b972b17f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label_fuzzy(\"RCT-drug-intervention\", label_to_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ae98-24fd-427f-b649-edbb5718fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_numerical(label, label_dict):\n",
    "    # Check if label is a dictionary\n",
    "    if isinstance(label, dict):\n",
    "        print(label)\n",
    "        # Extract the label with the highest score/probability\n",
    "        highest_label = max(normalized_label, key=label.get)\n",
    "        return label_dict.get(highest_label, -1)\n",
    "    else:\n",
    "         # Normalize label\n",
    "        normalized_label = label.replace(',', '').strip().replace(' ', '-')#.strip()\n",
    "        # Directly map string labels to numerical IDs\n",
    "        numerical_label = label_dict.get(label, -1)\n",
    "        # Fuzzy match if no direct mapping possible\n",
    "        if numerical_label == -1:\n",
    "            numerical_label = map_label_fuzzy(normalized_label, label_dict)\n",
    "        # If fuzzy match did not work, check if the label string contains the key 'label' and use it to split the string; keep everything to the right as the potential label\n",
    "        if numerical_label == -1:\n",
    "            if 'label' in label:\n",
    "                label_part = label.split('label')[1]\n",
    "                numerical_label = map_label_fuzzy(label_part, label_dict)\n",
    "        return numerical_label\n",
    "        \n",
    "# Convert accepted labels to numerical\n",
    "df['accepted_label_numerical'] = df['accepted_label'].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "\n",
    "for col in prediction_columns:\n",
    "    df[f'{col}_numerical'] = df[col].apply(lambda x: map_label_to_numerical(x, label_to_numerical))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41037782-adf1-4715-99d0-c25423b4afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"predictions/{model}_enriched_kw_test_test_outputs_{'_'.join(prompt_ids_to_eval)}_structured.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1922f74-aa1a-4ff2-aa6c-4dd434be26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['pmid'] == 19812460]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5fbb-23c4-48b8-be3b-a700d0667854",
   "metadata": {},
   "source": [
    "#### Important: some labels from GPT could not be mapped to a target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a4b8-fe40-4b75-aa4b-47874029ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_minus_one = df[(df == -1).any(axis=1)]\n",
    "rows_with_minus_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c211f76-a138-41a2-a16a-97cdaf21a5bd",
   "metadata": {},
   "source": [
    "## Evaluate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de7219-5c25-4042-b2c1-236f65150fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, digits=3):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report_with_ci(y_true, y_pred, numerical_to_label_map=numerical_to_label, round_ndigits = digits)\n",
    "\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report)\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        report_df.set_index('class', inplace=True)\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_{prompt_id}_{eval_type}_with_ci.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_enriched_kw_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2c904-2156-4069-9141-398732642c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_col = 'accepted_label_numerical'\n",
    "eval_type = 'multi_label'\n",
    "all_reports_df, summary_df = evaluate_predictions_with_ci(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label, digits=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb3349-56a7-438f-81da-172bda7f9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf933f0c-b02b-4356-8b22-045d9fcb1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320f53e-2a57-440e-ac4b-290805dd7d18",
   "metadata": {},
   "source": [
    "## Format Results and Generate LateX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881ef34-39d9-4143-a809-4882ec809921",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-turbo-preview\"\n",
    "\n",
    "prompt_ids_to_eval = [\"P6\", \"P7\", \"P11_3\", \"P11_4\"]\n",
    "summary_gpt4_raw_1 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1\", \"P4_1\", \"P5\"]\n",
    "summary_gpt4_raw_2 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "summary_gpt4_raw = pd.concat([summary_gpt4_raw_1,summary_gpt4_raw_2]) \n",
    "summary_gpt4_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db59666-c9a5-4047-8fb0-1f36a0988021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prompt_ids_to_eval = [\"P5\"]\n",
    "summary_gpt3_raw_1 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P6\"]\n",
    "summary_gpt3_raw_2 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1\"]\n",
    "summary_gpt3_raw_3 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P7\"]\n",
    "summary_gpt3_raw_4 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "summary_gpt3_raw = pd.concat([summary_gpt3_raw_1,summary_gpt3_raw_2, summary_gpt3_raw_3, summary_gpt3_raw_4]) \n",
    "summary_gpt3_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32739f-f08b-4e9d-8a99-d1aec44e9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_to_concept = {\n",
    "    'P1': 'zero-shot',\n",
    "    'P4_1': 'CC',\n",
    "    'P5': 'CC',\n",
    "    'P6': 'CC',\n",
    "    'P7': 'CoT',\n",
    "    'P11_3': 'CoT + CC',\n",
    "    'P11_4': 'CoT + CC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ffb79-ff55-452c-bd25-a94511c35956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sorting function\n",
    "def custom_sort(prompt):\n",
    "    import re\n",
    "    # Extract numbers from the prompt\n",
    "    numbers = re.findall(r'\\d+', prompt)\n",
    "    if numbers:\n",
    "        # Primary sort by the first number, secondary sort by the full string\n",
    "        return (int(numbers[0]), prompt)\n",
    "    return (float('inf'), prompt)  # Handles cases without numbers\n",
    "    \n",
    "def format_summary_for_latex_report(summary_df, map_prompt_to_concept):\n",
    "    summary_df = summary_df.copy()\n",
    "    \n",
    "    # Concatenating each metric with its CI\n",
    "    summary_df['Precision (CI)'] = summary_df['precision'].astype(str) + ' ' + summary_df['precision CI'].astype(str)\n",
    "    summary_df['Recall (CI)'] = summary_df['recall'].astype(str) + ' ' + summary_df['recall CI'].astype(str)\n",
    "    summary_df['F1-Score (CI)'] = summary_df['f1-score'].astype(str) + ' ' + summary_df['f1-score CI'].astype(str)\n",
    "    summary_df['Accuracy (CI)'] = summary_df['accuracy'].astype(str) + ' ' + summary_df['accuracy CI'].astype(str)\n",
    "    \n",
    "    # Dropping old columns\n",
    "    summary_df.drop(columns=['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI'], inplace=True)\n",
    "    \n",
    "    # Rename 'Prompt ID' to 'Prompt'\n",
    "    summary_df.rename(columns={'Prompt ID': 'Prompt'}, inplace=True)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    summary_df['Concept'] = summary_df['Prompt'].map(map_prompt_to_concept)\n",
    "    \n",
    "    # Rearrange the columns to put 'Concept' after 'Prompt'\n",
    "    summary_df = summary_df[['Prompt', 'Concept', 'Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)']]\n",
    "\n",
    "    summary_df['sort_key'] = summary_df['Prompt'].apply(custom_sort)\n",
    "    summary_df.sort_values('sort_key', inplace=True)\n",
    "    summary_df.drop('sort_key', inplace=True, axis=1)\n",
    "    \n",
    "    return summary_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b34acb-1c4d-4e8d-8f74-9318eb8fcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt4 = format_summary_for_latex_report(summary_gpt4_raw, map_prompt_to_concept)\n",
    "summary_gpt4['Model'] = 'gpt-4'\n",
    "summary_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34925dd0-2db5-47e1-9913-df4beb37fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_gpt4.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2be62-9f1d-4a62-a807-dd1ead802c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt3 = format_summary_for_latex_report(summary_gpt3_raw, map_prompt_to_concept)\n",
    "summary_gpt3['Model'] = 'gpt-3.5'\n",
    "summary_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511106a-7789-4bdb-8064-fd01214fb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_gpt3.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edafef-ccd6-4601-ad34-d5817128463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([summary_gpt4, summary_gpt3])\n",
    "\n",
    "# Pivot table with multi-level columns for metrics and models as subcolumns\n",
    "pivot_df = combined_df.pivot_table(\n",
    "    index=['Prompt', 'Concept'],\n",
    "    columns='Model',\n",
    "    values=['Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Simplify the MultiIndex in columns\n",
    "pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "pivot_df['sort_key'] = pivot_df['Prompt'].apply(custom_sort)\n",
    "pivot_df.sort_values('sort_key', inplace=True)\n",
    "pivot_df.drop('sort_key', inplace=True, axis=1)\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6d9d0-f433-4b20-ac9d-21c2e5c35ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivot_df.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e438619-6936-49e0-8359-1a52ae657199",
   "metadata": {},
   "source": [
    "### archive for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f81701-9ac1-4137-afa0-49ee1b5a7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0, labels=range(len(label_to_numerical)), target_names=[numerical_to_label[i] for i in range(len(label_to_numerical))])\n",
    "\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'recall', 'f1-score']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_{prompt_id}_{eval_type}.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_enriched_kw_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #all_reports_df.to_csv(f\"evaluations/{model}_enriched_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #summary_df.to_csv(f\"evaluations/{model}_enriched_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
