{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e861e18f-284f-4bd3-a15d-c2d83f82f090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e83a32-ca98-485b-b374-1ffb753cf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confidenceinterval import classification_report_with_ci\n",
    "from confidenceinterval.bootstrap import bootstrap_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef38918-a01e-4d38-b932-653ba5298277",
   "metadata": {},
   "source": [
    "### Format Results and Generate LateX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b455b5d-a4d5-4ebe-ba85-3d666fc5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-turbo-preview\"\n",
    "\n",
    "prompt_ids_to_eval = [\"P6\", \"P7\", \"P11_3\", \"P11_4\"]\n",
    "summary_gpt4_raw_1 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1\", \"P4_1\", \"P5\"]\n",
    "summary_gpt4_raw_2 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "summary_gpt4_raw = pd.concat([summary_gpt4_raw_1,summary_gpt4_raw_2]) \n",
    "summary_gpt4_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8926a8-1b9d-4ebc-bfe9-8640a6cdc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prompt_ids_to_eval = [\"P5\"]\n",
    "summary_gpt3_raw_1 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "#name must be gpt3, because with gpt3.5 some format/meaning change happens\n",
    "\n",
    "prompt_ids_to_eval = [\"P6\"]\n",
    "summary_gpt3_raw_2 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P1\"]\n",
    "summary_gpt3_raw_3 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "prompt_ids_to_eval = [\"P7\"]\n",
    "summary_gpt3_raw_4 = pd.read_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}_with_ci.csv\", index_col=0)\n",
    "\n",
    "summary_gpt3_raw = pd.concat([summary_gpt3_raw_1,summary_gpt3_raw_2, summary_gpt3_raw_3, summary_gpt3_raw_4]) \n",
    "summary_gpt3_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20048cfe-b061-4726-9a0a-f7afb6a94e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_to_concept = {\n",
    "    'P1': 'zero-shot',\n",
    "    'P4_1': 'CC',\n",
    "    'P5': 'CC',\n",
    "    'P6': 'CC',\n",
    "    'P7': 'CoT',\n",
    "    'P11_3': 'CoT + CC',\n",
    "    'P11_4': 'CoT + CC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fe17b-3ccb-4d31-8526-104d5bb29cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sorting function\n",
    "def custom_sort(prompt):\n",
    "    import re\n",
    "    # Extract numbers from the prompt\n",
    "    numbers = re.findall(r'\\d+', prompt)\n",
    "    if numbers:\n",
    "        # Primary sort by the first number, secondary sort by the full string\n",
    "        return (int(numbers[0]), prompt)\n",
    "    return (float('inf'), prompt)  # Handles cases without numbers\n",
    "    \n",
    "def format_summary_for_latex_report(summary_df, map_prompt_to_concept):\n",
    "    summary_df = summary_df.copy()\n",
    "    \n",
    "    # Concatenating each metric with its CI\n",
    "    summary_df['Precision (CI)'] = summary_df['precision'].astype(str) + ' ' + summary_df['precision CI'].astype(str)\n",
    "    summary_df['Recall (CI)'] = summary_df['recall'].astype(str) + ' ' + summary_df['recall CI'].astype(str)\n",
    "    summary_df['F1-Score (CI)'] = summary_df['f1-score'].astype(str) + ' ' + summary_df['f1-score CI'].astype(str)\n",
    "    summary_df['Accuracy (CI)'] = summary_df['accuracy'].astype(str) + ' ' + summary_df['accuracy CI'].astype(str)\n",
    "    \n",
    "    # Dropping old columns\n",
    "    summary_df.drop(columns=['precision', 'precision CI', 'recall', 'recall CI', 'f1-score', 'f1-score CI', 'accuracy', 'accuracy CI'], inplace=True)\n",
    "    \n",
    "    # Rename 'Prompt ID' to 'Prompt'\n",
    "    summary_df.rename(columns={'Prompt ID': 'Prompt'}, inplace=True)\n",
    "    \n",
    "    # Apply the mapping\n",
    "    summary_df['Concept'] = summary_df['Prompt'].map(map_prompt_to_concept)\n",
    "    \n",
    "    # Rearrange the columns to put 'Concept' after 'Prompt'\n",
    "    summary_df = summary_df[['Prompt', 'Concept', 'Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)']]\n",
    "\n",
    "    summary_df['sort_key'] = summary_df['Prompt'].apply(custom_sort)\n",
    "    summary_df.sort_values('sort_key', inplace=True)\n",
    "    summary_df.drop('sort_key', inplace=True, axis=1)\n",
    "    \n",
    "    return summary_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bf3c8-6e45-4e4e-bb20-a6808ab4868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt4 = format_summary_for_latex_report(summary_gpt4_raw, map_prompt_to_concept)\n",
    "summary_gpt4['Model'] = 'gpt-4'\n",
    "summary_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739ef44-99ea-42c0-bb52-35310f45669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_gpt4.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ada948-fafd-4659-b5aa-8837bf6d6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gpt3 = format_summary_for_latex_report(summary_gpt3_raw, map_prompt_to_concept)\n",
    "summary_gpt3['Model'] = 'gpt-3.5'\n",
    "summary_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef8548-4ca7-4f6c-91b9-29243ab01f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_gpt3.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10feeb6-3188-4b5e-9d5c-4922d8a77108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([summary_gpt4, summary_gpt3])\n",
    "\n",
    "# Pivot table with multi-level columns for metrics and models as subcolumns\n",
    "pivot_df = combined_df.pivot_table(\n",
    "    index=['Prompt', 'Concept'],\n",
    "    columns='Model',\n",
    "    values=['Precision (CI)', 'Recall (CI)', 'F1-Score (CI)', 'Accuracy (CI)'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Simplify the MultiIndex in columns\n",
    "pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "pivot_df['sort_key'] = pivot_df['Prompt'].apply(custom_sort)\n",
    "pivot_df.sort_values('sort_key', inplace=True)\n",
    "pivot_df.drop('sort_key', inplace=True, axis=1)\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660ee6c-05aa-4fd7-b42c-3de4d6580dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivot_df.to_latex(float_format=\"%.3f\", index=False, formatters={'Prompt': lambda x: x.replace('_', r'\\_')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a47553-3260-4e75-8980-3188c2a2eb09",
   "metadata": {},
   "source": [
    "### archive for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d528d-c7ea-4d90-be74-49d44e3519e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(df, target_label_col, prompt_ids_to_eval, model, eval_type, label_to_numerical, numerical_to_label):\n",
    "    report_dfs = []\n",
    "    summary_stats = []\n",
    "\n",
    "    for prompt_id in prompt_ids_to_eval:\n",
    "        print(\"Evaluating \", prompt_id)\n",
    "        prediction_col = f'gpt_predictions_{prompt_id}'\n",
    "\n",
    "        # Extract arrays for evaluation\n",
    "        y_true = df[target_label_col].values\n",
    "        y_pred = df[f'{prediction_col}_numerical'].values\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(len(label_to_numerical)))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        accuracy_balanced = balanced_accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0, labels=range(len(label_to_numerical)), target_names=[numerical_to_label[i] for i in range(len(label_to_numerical))])\n",
    "\n",
    "        \n",
    "        # Create DataFrame from report\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df['Prompt ID'] = prompt_id\n",
    "        report_dfs.append(report_df)\n",
    "        \n",
    "        # Extract summary statistics\n",
    "        summary = report_df.loc['weighted avg', ['precision', 'recall', 'f1-score']].to_dict()\n",
    "        summary['Prompt ID'] = prompt_id\n",
    "        summary_stats.append(summary)\n",
    "\n",
    "        # Plotting confusion matrix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(len(label_to_numerical)), yticklabels=range(len(label_to_numerical)))\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=13)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=13)\n",
    "        plt.title(f'Confusion Matrix for Model {model} and Prompt {prompt_id}', fontsize=14)\n",
    "        plt.xlabel('Predicted Labels', fontsize=13)\n",
    "        plt.ylabel('True Labels', fontsize=13)\n",
    "\n",
    "        # Add an inset with label mapping\n",
    "        textstr = '\\n'.join([f'{v}: {k}' for k, v in label_to_numerical.items()])\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.16, 1.0, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/confusion_matrix_{model}_{prompt_id}_{eval_type}.pdf')  # Save to PDF\n",
    "\n",
    "    # Combine all report DataFrames\n",
    "    all_reports_df = pd.concat(report_dfs)\n",
    "\n",
    "    # Create a summary table for average precision, recall, and F1-score\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    # Save results to CSV files\n",
    "    all_reports_df.to_csv(f\"evaluations/{model}_enriched_kw_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    summary_df.to_csv(f\"evaluations/{model}_enriched_kw_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #all_reports_df.to_csv(f\"evaluations/{model}_enriched_test_per_class_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    #summary_df.to_csv(f\"evaluations/{model}_enriched_test_summary_{'_'.join(prompt_ids_to_eval)}_{eval_type}.csv\")\n",
    "    \n",
    "    print(\"Results saved to evaluations/ and plots/ folders.\")\n",
    "\n",
    "    return all_reports_df, summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
